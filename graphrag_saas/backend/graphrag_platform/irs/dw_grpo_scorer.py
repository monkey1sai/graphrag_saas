"""DW-GRPO Scorerï¼šå‹•æ…‹æ¬Šé‡ + CPU Offload

ç”¨æ–¼ IRS (Iterative Rejection Sampling) çš„è©•åˆ†éšæ®µï¼š
- r_rel (ç›¸é—œæ€§)ï¼šCross-Encoder reranker
- r_faith (å¿ å¯¦åº¦)ï¼šEmbedding cosine similarity (BERTScore æ›¿ä»£)
- r_conc (ç°¡æ½”æ€§)ï¼šé•·åº¦æ¯” heuristic

å‹•æ…‹æ¬Šé‡æ›´æ–°ï¼š
- ç›£æ¸¬å„æŒ‡æ¨™çš„æ»‘å‹•çª—å£æ–œç‡
- æ–œç‡è¶Šå°ï¼ˆåœæ»¯ï¼‰æ¬Šé‡è¶Šå¤§
- ä½¿ç”¨ softmax æ­¸ä¸€åŒ– + momentum å¹³æ»‘æ›´æ–°
"""

from __future__ import annotations

import os
from dataclasses import dataclass, field
from typing import Any

import numpy as np

# å¯é¸ä¾è³´ï¼šsentence-transformers / bert-score
try:
    from sentence_transformers import CrossEncoder, SentenceTransformer

    ST_AVAILABLE = True
except ImportError:
    ST_AVAILABLE = False
    CrossEncoder = None  # type: ignore
    SentenceTransformer = None  # type: ignore

try:
    import torch

    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None  # type: ignore


@dataclass
class ScoredCandidate:
    """å–®ä¸€å€™é¸ç­”æ¡ˆçš„è©•åˆ†çµæœ"""

    candidate: str
    r_rel: float
    r_faith: float
    r_conc: float
    weighted_score: float
    rank: int = 0


@dataclass
class SelectionResult:
    """Best-of-N é¸æ“‡çµæœ"""

    query: str
    context: str
    best: ScoredCandidate
    all_candidates: list[ScoredCandidate]
    current_weights: dict[str, float]
    avg_rewards: dict[str, float]


@dataclass
class DWGRPOConfig:
    """DW-GRPO è©•åˆ†å™¨é…ç½®"""

    device: str = "cpu"
    history_window: int = 5
    temperature: float = 2.0
    momentum: float = 0.8
    initial_weights: dict[str, float] = field(
        default_factory=lambda: {"rel": 1.0, "faith": 1.0, "conc": 0.5}
    )
    # æ¨¡å‹ ID
    reranker_model: str = "BAAI/bge-reranker-v2-m3"
    encoder_model: str = "BAAI/bge-m3"


class DWGRPOScorer:
    """DW-GRPO å‹•æ…‹æ¬Šé‡è©•åˆ†å™¨ï¼ˆCPU Offload ç‰ˆï¼‰

    é©ç”¨æ–¼ 8GB VRAM ç’°å¢ƒï¼š
    - Cross-Encoder reranker åœ¨ CPU åŸ·è¡Œ
    - Embedding model åœ¨ CPU åŸ·è¡Œ
    - åˆ©ç”¨ 64GB RAM å„ªå‹¢
    """

    def __init__(self, config: DWGRPOConfig | None = None) -> None:
        self.config = config or DWGRPOConfig(
            device=os.getenv("REWARD_DEVICE", "cpu")
        )

        # æ¬Šé‡èˆ‡æ­·å²
        self.weights = dict(self.config.initial_weights)
        self.history: dict[str, list[float]] = {k: [] for k in self.weights}

        # å»¶é²è¼‰å…¥æ¨¡å‹
        self._reranker: CrossEncoder | None = None
        self._encoder: SentenceTransformer | None = None
        self._models_loaded = False

    def _ensure_models_loaded(self) -> None:
        """å»¶é²è¼‰å…¥ CPU Reward Models"""
        if self._models_loaded:
            return

        if not ST_AVAILABLE:
            raise ImportError(
                "DWGRPOScorer requires 'sentence-transformers'. "
                "Install with: pip install sentence-transformers"
            )

        device = self.config.device
        print(f"ğŸ¢ Loading Reward Models on {device}...")

        # Cross-Encoder for r_rel
        self._reranker = CrossEncoder(self.config.reranker_model, device=device)

        # SentenceTransformer for r_faith
        self._encoder = SentenceTransformer(self.config.encoder_model, device=device)

        self._models_loaded = True
        print("âœ… Reward Models loaded successfully.")

    def compute_rewards(
        self, query: str, context: str, candidates: list[str]
    ) -> list[dict[str, float]]:
        """è¨ˆç®—æ¯å€‹å€™é¸ç­”æ¡ˆçš„ä¸‰é …åŸå§‹åˆ†æ•¸"""
        self._ensure_models_loaded()

        rewards_list: list[dict[str, float]] = []

        for cand in candidates:
            # r_rel: ç›¸é—œæ€§ï¼ˆCross-Encoderï¼‰
            r_rel = float(self._reranker.predict([(query, cand)])[0])  # type: ignore

            # r_faith: å¿ å¯¦åº¦ï¼ˆEmbedding Cosine Similarityï¼‰
            # ä½¿ç”¨ embedding ç›¸ä¼¼åº¦ä½œç‚º BERTScore å¿«é€Ÿæ›¿ä»£
            cand_emb = self._encoder.encode(  # type: ignore
                cand, convert_to_tensor=True, show_progress_bar=False
            )
            ctx_emb = self._encoder.encode(  # type: ignore
                context, convert_to_tensor=True, show_progress_bar=False
            )

            if TORCH_AVAILABLE and torch is not None:
                r_faith = float(
                    torch.cosine_similarity(cand_emb, ctx_emb, dim=0).item()
                )
            else:
                # Fallback: numpy cosine similarity
                cand_np = np.array(cand_emb)
                ctx_np = np.array(ctx_emb)
                r_faith = float(
                    np.dot(cand_np, ctx_np)
                    / (np.linalg.norm(cand_np) * np.linalg.norm(ctx_np) + 1e-8)
                )

            # r_conc: ç°¡æ½”æ€§ï¼ˆé•·åº¦æ‡²ç½°ï¼‰
            # r_conc = max(0, 1 - len(C)/len(K))
            r_conc = max(0.0, 1.0 - (len(cand) / (len(context) + 1e-5)))

            rewards_list.append({"rel": r_rel, "faith": r_faith, "conc": r_conc})

        return rewards_list

    def select_best(
        self, query: str, context: str, candidates: list[str]
    ) -> SelectionResult:
        """DW-GRPO æ ¸å¿ƒæµç¨‹ï¼šè¨ˆç®—åˆ†æ•¸ â†’ æ›´æ–°æ¬Šé‡ â†’ é¸æ“‡æœ€ä½³"""
        # 1. è¨ˆç®—åŸå§‹çå‹µ
        raw_rewards = self.compute_rewards(query, context, candidates)

        # 2. è¨ˆç®—åŠ æ¬Šç¸½åˆ†
        scored: list[ScoredCandidate] = []
        for idx, (cand, r) in enumerate(zip(candidates, raw_rewards, strict=False)):
            weighted = (
                self.weights["rel"] * r["rel"]
                + self.weights["faith"] * r["faith"]
                + self.weights["conc"] * r["conc"]
            )
            scored.append(
                ScoredCandidate(
                    candidate=cand,
                    r_rel=r["rel"],
                    r_faith=r["faith"],
                    r_conc=r["conc"],
                    weighted_score=weighted,
                    rank=0,
                )
            )

        # 3. æ’åºä¸¦é¸æ“‡æœ€ä½³
        scored.sort(key=lambda x: x.weighted_score, reverse=True)
        for rank, s in enumerate(scored):
            s.rank = rank + 1

        best = scored[0]

        # 4. è¨˜éŒ„æ­·å²ï¼ˆä½¿ç”¨å¹³å‡å€¼ä»£è¡¨æœ¬è¼ªè¡¨ç¾ï¼‰
        avg_rewards = {
            k: float(np.mean([r[k] for r in raw_rewards])) for k in self.weights
        }
        for k, v in avg_rewards.items():
            self.history[k].append(v)

        # 5. å‹•æ…‹æ¬Šé‡æ›´æ–°
        self._update_weights()

        return SelectionResult(
            query=query,
            context=context,
            best=best,
            all_candidates=scored,
            current_weights=dict(self.weights),
            avg_rewards=avg_rewards,
        )

    def _update_weights(self) -> None:
        """æ ¹æ“šæ­·å²æ–œç‡èª¿æ•´æ¬Šé‡ï¼šæå‡æ…¢çš„æŒ‡æ¨™æ¬Šé‡å¢åŠ """
        window = self.config.history_window
        if len(self.history["rel"]) < window:
            return

        slopes: dict[str, float] = {}
        for k, vals in self.history.items():
            # è¨ˆç®—æœ€è¿‘ N è¼ªçš„è®ŠåŒ–æ–œç‡ï¼ˆLinear Regression Slopeï¼‰
            y = np.array(vals[-window:])
            x = np.arange(len(y))
            slope = float(np.polyfit(x, y, 1)[0])
            slopes[k] = slope

        # DW-GRPO æ ¸å¿ƒï¼šæ–œç‡è¶Šå°ï¼ˆstagnantï¼‰ï¼Œæ¬Šé‡æ‡‰è¶Šå¤§
        # ä½¿ç”¨ Softmax æ­¸ä¸€åŒ–åå‘æ–œç‡
        # w_j = exp(-slope_j / T) / sum(...)
        T = self.config.temperature
        exps = {k: np.exp(-v / T) for k, v in slopes.items()}
        total = sum(exps.values())

        # ä¹˜ 3 ä¿æŒç¸½æ¬Šé‡ç´„ç‚º 3ï¼ˆèˆ‡åˆå§‹æ¬Šé‡ç¸½å’Œä¸€è‡´ï¼‰
        new_weights = {k: 3.0 * (v / total) for k, v in exps.items()}

        # Momentum å¹³æ»‘æ›´æ–°
        alpha = self.config.momentum
        self.weights = {
            k: alpha * self.weights[k] + (1 - alpha) * new_weights[k]
            for k in self.weights
        }

    def get_state(self) -> dict[str, Any]:
        """å–å¾—ç•¶å‰ç‹€æ…‹ï¼ˆç”¨æ–¼æŒä¹…åŒ–/å›å ±ï¼‰"""
        return {
            "weights": dict(self.weights),
            "history_length": {k: len(v) for k, v in self.history.items()},
            "models_loaded": self._models_loaded,
            "device": self.config.device,
        }

    def reset(self) -> None:
        """é‡ç½®æ¬Šé‡èˆ‡æ­·å²"""
        self.weights = dict(self.config.initial_weights)
        self.history = {k: [] for k in self.weights}


class HeuristicDWGRPOScorer:
    """è¼•é‡ç´š DW-GRPO Scorerï¼ˆä¸éœ€è¦ sentence-transformersï¼‰

    ç”¨æ–¼ï¼š
    - å¿«é€Ÿæ¸¬è©¦
    - ç„¡ ML ä¾è³´ç’°å¢ƒ
    - Fallback å ´æ™¯
    """

    def __init__(self, config: DWGRPOConfig | None = None) -> None:
        self.config = config or DWGRPOConfig()
        self.weights = dict(self.config.initial_weights)
        self.history: dict[str, list[float]] = {k: [] for k in self.weights}

    def compute_rewards(
        self, query: str, context: str, candidates: list[str]
    ) -> list[dict[str, float]]:
        """ä½¿ç”¨ heuristic è¨ˆç®—çå‹µï¼ˆç„¡ ML æ¨¡å‹ï¼‰"""
        rewards_list: list[dict[str, float]] = []

        query_tokens = set(query.lower().split())
        context_tokens = set(context.lower().split())

        for cand in candidates:
            cand_tokens = set(cand.lower().split())

            # r_rel: èˆ‡ query çš„ token overlap
            if query_tokens:
                r_rel = len(cand_tokens & query_tokens) / len(query_tokens)
            else:
                r_rel = 0.0

            # r_faith: èˆ‡ context çš„ token overlap
            if context_tokens:
                r_faith = len(cand_tokens & context_tokens) / len(context_tokens)
            else:
                r_faith = 0.0

            # r_conc: é•·åº¦æ¯”
            r_conc = max(0.0, 1.0 - (len(cand) / (len(context) + 1e-5)))

            rewards_list.append({"rel": r_rel, "faith": r_faith, "conc": r_conc})

        return rewards_list

    def select_best(
        self, query: str, context: str, candidates: list[str]
    ) -> SelectionResult:
        """åŒ DWGRPOScorer ä½†ä½¿ç”¨ heuristic rewards"""
        raw_rewards = self.compute_rewards(query, context, candidates)

        scored: list[ScoredCandidate] = []
        for cand, r in zip(candidates, raw_rewards, strict=False):
            weighted = (
                self.weights["rel"] * r["rel"]
                + self.weights["faith"] * r["faith"]
                + self.weights["conc"] * r["conc"]
            )
            scored.append(
                ScoredCandidate(
                    candidate=cand,
                    r_rel=r["rel"],
                    r_faith=r["faith"],
                    r_conc=r["conc"],
                    weighted_score=weighted,
                )
            )

        scored.sort(key=lambda x: x.weighted_score, reverse=True)
        for rank, s in enumerate(scored):
            s.rank = rank + 1

        avg_rewards = {
            k: float(np.mean([r[k] for r in raw_rewards])) for k in self.weights
        }
        for k, v in avg_rewards.items():
            self.history[k].append(v)

        self._update_weights()

        return SelectionResult(
            query=query,
            context=context,
            best=scored[0],
            all_candidates=scored,
            current_weights=dict(self.weights),
            avg_rewards=avg_rewards,
        )

    def _update_weights(self) -> None:
        """åŒ DWGRPOScorer çš„æ¬Šé‡æ›´æ–°é‚è¼¯"""
        window = self.config.history_window
        if len(self.history["rel"]) < window:
            return

        slopes = {}
        for k, vals in self.history.items():
            y = np.array(vals[-window:])
            x = np.arange(len(y))
            slope = float(np.polyfit(x, y, 1)[0])
            slopes[k] = slope

        T = self.config.temperature
        exps = {k: np.exp(-v / T) for k, v in slopes.items()}
        total = sum(exps.values())
        new_weights = {k: 3.0 * (v / total) for k, v in exps.items()}

        alpha = self.config.momentum
        self.weights = {
            k: alpha * self.weights[k] + (1 - alpha) * new_weights[k]
            for k in self.weights
        }

    def get_state(self) -> dict[str, Any]:
        return {
            "weights": dict(self.weights),
            "history_length": {k: len(v) for k, v in self.history.items()},
            "type": "heuristic",
        }

    def reset(self) -> None:
        self.weights = dict(self.config.initial_weights)
        self.history = {k: [] for k in self.weights}


def get_default_scorer(config: DWGRPOConfig | None = None) -> DWGRPOScorer | HeuristicDWGRPOScorer:
    """å–å¾—é è¨­ scorerï¼ˆè‡ªå‹•é¸æ“‡ model-based æˆ– heuristicï¼‰"""
    import os

    mode = os.getenv("DWGRPO_SCORER", "").strip().lower()
    if mode in {"heuristic", "simple"}:
        return HeuristicDWGRPOScorer(config)
    if mode in {"model", "st", "sentence-transformers"}:
        if not ST_AVAILABLE:
            raise RuntimeError("DWGRPO_SCORER=model requested but sentence-transformers is not available.")
        return DWGRPOScorer(config)

    if ST_AVAILABLE:
        return DWGRPOScorer(config)
    else:
        print("âš ï¸ sentence-transformers not available, using HeuristicDWGRPOScorer")
        return HeuristicDWGRPOScorer(config)
