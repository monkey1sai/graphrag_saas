The Text‑to‑Text Transfer Transformer (T5) introduces a unified framework that converts every language problem into a text‑to‑text format. Through a systematic study of pre‑training objectives, model architectures, unlabeled datasets and transfer techniques, the authors explore the landscape of transfer learning in natural language processing. By scaling the model and training on the Colossal Clean Crawled Corpus (C4), T5 achieves state‑of‑the‑art results on summarization, question answering, text classification and other benchmarks. The authors release the data, pre‑trained models and code to facilitate further research.