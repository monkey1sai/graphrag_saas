Learning Transferable Visual Models From Natural Language Supervision (CLIP) jointly trains an image encoder and a text encoder on 400 million image–text pairs collected from the internet. A simple pre‑training task of predicting which caption matches which image produces general visual representations that can be accessed via natural language. After pre‑training, natural language prompts enable zero‑shot transfer of the model to a wide range of downstream computer vision tasks without task‑specific training, matching or surpassing the performance of a ResNet‑50 baseline on ImageNet and other benchmarks.