The Vision Transformer (ViT) applies a pure Transformer architecture to image recognition by dividing an image into a sequence of patches and feeding them to a standard Transformer. When pre‑trained on large datasets and fine‑tuned on smaller benchmarks, ViT achieves excellent results and often surpasses convolutional networks while requiring substantially fewer computational resources. The authors show that large‑scale training can overcome the lack of convolutional inductive biases, and that ViT reaches state‑of‑the‑art accuracies on ImageNet, CIFAR‑100 and other benchmarks when trained on millions of images.