Adam combines the benefits of AdaGrad and RMSProp, provides invariance to rescaling of gradients, and works well for non‑stationary or sparse objectives, consistently outperforming other first‑order methods in empirical evaluations. It is straightforward to implement, computationally efficient and requires little memory, making it suitable for large‑scale problems. Adam computes adaptive learning rates for each parameter using estimates of the first and second moments of gradients, providing a stochastic optimisation algorithm that works well across a wide range of problems.